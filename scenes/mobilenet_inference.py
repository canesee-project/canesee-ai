# -*- coding: utf-8 -*-
"""mobilenet_inference(1)_beam_search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kplPTi5xInAK3Ebd2o0qtc81sYNJ65c
"""

# Commented out IPython magic to ensure Python compatibility.
#import packages

import tensorflow as tf

import numpy as np
import pickle
from PIL import Image

#import required classes

from BahdanauAttentionTest import BahdanauAttentionTest
from rnn_Decoder import RNN_DecoderTest
from CNN_Encoder import CNN_Encoder

#!pip install tensorflow==2.0

tf.__version__

#max_length of_train_sequences
max_length=46

# Feel free to change these parameters according to your system's configuration

BATCH_SIZE = 32
BUFFER_SIZE = 1000
embedding_dim = 256
units = 512
len_tokenizer_word_index=26555
vocab_size = len_tokenizer_word_index + 1
len_img_name_train=80000
num_steps = len_img_name_train // BATCH_SIZE
# Shape of the vector extracted from mobilenetv2 is (49, 1792)
# These two variables represent that vector shape
features_shape = 1792
attention_features_shape = 49

#weights folder path
path_w = 'weights/'

encoder = CNN_Encoder(embedding_dim)
encoder.build(input_shape=(49,1792))
encoder.load_weights(path_w+"encoder_weights_100k_ar.h5")
decoder = RNN_DecoderTest(embedding_dim, units, vocab_size)

image_model=tf.keras.applications.MobileNetV2(include_top=False, weights='mobilenet_v2_weights_1.4.h5' ,alpha=1.4 )
new_input=image_model.input
hidden_layer=image_model.layers[-1].output
image_features_extract_model=tf.keras.Model(new_input,hidden_layer)

def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (224, 224))
    img = tf.keras.applications.mobilenet_v2.preprocess_input(img) 
    return img, image_path

#plot_attention(image_path, result, attention_plot)

def evaluate_b(image, beam_index ):
    infile = open('tokenizer.pickle','rb')
    tokenizer= pickle.load(infile)
    infile.close()

    start = [tokenizer.word_index['<start>']]
    
    # result[0][0] = index of the starting word
    # result[0][1] = probability of the word predicted
    result = [[start, 0.0]]

    attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)

    while len(result[0][0]) < max_length:
        i=0
        temp = []
        for s in result:

          predictions, hidden, attention_weights = decoder(dec_input, features, hidden)

          attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()
          i=i+1
          # Getting the top <beam_index>(n) predictions
          word_preds = np.argsort(predictions[0])[-beam_index:]
          
          # creating a new list so as to put them via the model again
          for w in word_preds:       
            next_cap, prob = s[0][:], s[1]
            next_cap.append(w)
            prob += predictions[0][w]
            temp.append([next_cap, prob])
        result = temp
        # Sorting according to the probabilities
        result = sorted(result, reverse=False, key=lambda l: l[1])
        # Getting the top words
        result = result[-beam_index:]
        
        predicted_id = result[-1] # with Max Probability
        pred_list = predicted_id[0]
        
        prd_id = pred_list[-1] 
        if(prd_id!=3):
          dec_input = tf.expand_dims([prd_id], 0)  # Decoder input is the word predicted with highest probability among the top_k words predicted
        else:
          break

    result = result[-1][0]
    
    intermediate_caption = [tokenizer.index_word[i] for i in result]
    final_caption = []
    for i in intermediate_caption:
        if i != '<end>':
            final_caption.append(i)
            
        else:
            break

    attention_plot = attention_plot[:len(result), :]
    final_caption = ' '.join(final_caption[1:])
    return final_caption



def evaluateTest(image):
    attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)
    infile = open('tokenizer.pickle','rb')
    toketokenizer= pickle.load(infile)
    infile.close()
    
    dec_input = tf.expand_dims([toketokenizer.word_index["<start>"]], 0)
   # print(tokenizer.word_index["<start>"])
    result = []
   # print("SHAPES", dec_input.shape, features.shape, hidden.shape)
    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)

        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.argmax(predictions[0]).numpy()
        #HERE
        result.append(toketokenizer.index_word[predicted_id])
#HERE
        if toketokenizer.index_word[predicted_id] == '<end>':
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot



image_path="test33.jpg"
Image.open(image_path)

print ("Beam search caption:")
#the beam_index is changed untill we get the best result
print(evaluate_b(image_path,beam_index=7))


result, attention_plot = evaluateTest(image_path)
print ('Greedy Caption:', ' '.join(result))

